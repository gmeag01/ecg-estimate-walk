{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.signal import find_peaks\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pickle\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "np.random.seed(datetime.datetime.now().microsecond)\n",
    "\n",
    "PEAK_THRESHOLD = 390\n",
    "DATA_PATH = \"./data/exported/30sec/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sitting data load\n",
    "with gzip.open(DATA_PATH + \"sitting_ecg.pkl\", \"rb\") as f:\n",
    "    sitting_ecg = pickle.load(f)\n",
    "\n",
    "with gzip.open(DATA_PATH + \"sitting_acc.pkl\", \"rb\") as f:\n",
    "    sitting_acc = pickle.load(f)\n",
    "sitting_acc = sitting_acc - np.mean(sitting_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(sitting_acc), 1, figsize=(20, 10))\n",
    "for idx in range(len(sitting_acc)):\n",
    "    peaks, _ = find_peaks(sitting_acc[idx], height=PEAK_THRESHOLD)\n",
    "    np.diff(peaks)\n",
    "\n",
    "    ax[idx].plot(sitting_acc[idx])\n",
    "    ax[idx].plot(peaks, sitting_acc[idx][peaks], \"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_list = list()\n",
    "for idx in range(len(sitting_acc)):\n",
    "    peaks, _ = find_peaks(sitting_acc[idx], height=PEAK_THRESHOLD)\n",
    "    if peaks == []:\n",
    "        peak_list.append(np.array([0]))\n",
    "    peak_list.append(len(peaks))\n",
    "\n",
    "X_sitting = sitting_ecg\n",
    "y_sitting = peak_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walking data load\n",
    "with gzip.open(DATA_PATH + \"walking_ecg.pkl\", \"rb\") as f:\n",
    "    walking_ecg = pickle.load(f)\n",
    "\n",
    "with gzip.open(DATA_PATH + \"walking_acc.pkl\", \"rb\") as f:\n",
    "    walking_acc = pickle.load(f)\n",
    "walking_acc = walking_acc - np.mean(walking_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(walking_acc), 1, figsize=(20, 20))\n",
    "for idx in range(len(walking_acc)):\n",
    "    peaks, _ = find_peaks(walking_acc[idx], height=PEAK_THRESHOLD)\n",
    "    np.diff(peaks)\n",
    "\n",
    "    ax[idx].plot(walking_acc[idx])\n",
    "    ax[idx].plot(peaks, walking_acc[idx][peaks], \"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_list = list()\n",
    "for idx in range(len(walking_acc)):\n",
    "    peaks, _ = find_peaks(walking_acc[idx], height=PEAK_THRESHOLD)\n",
    "    if peaks == []:\n",
    "        peak_list.append(np.array([0]))\n",
    "    peak_list.append(len(peaks))\n",
    "\n",
    "X_walking = walking_ecg\n",
    "y_walking = peak_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_walking)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running data load\n",
    "with gzip.open(DATA_PATH + \"running_ecg.pkl\", \"rb\") as f:\n",
    "    running_ecg = pickle.load(f)\n",
    "\n",
    "with gzip.open(DATA_PATH + \"running_acc.pkl\", \"rb\") as f:\n",
    "    running_acc = pickle.load(f)\n",
    "running_acc = running_acc - np.mean(running_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(running_acc), 1, figsize=(20, 10))\n",
    "for idx in range(len(running_acc)):\n",
    "    peaks, _ = find_peaks(running_acc[idx], height=PEAK_THRESHOLD)\n",
    "    np.diff(peaks)\n",
    "\n",
    "    ax[idx].plot(running_acc[idx])\n",
    "    ax[idx].plot(peaks, running_acc[idx][peaks], \"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_list = list()\n",
    "for idx in range(len(running_acc)):\n",
    "    peaks, _ = find_peaks(running_acc[idx], height=PEAK_THRESHOLD)\n",
    "    if peaks == []:\n",
    "        peak_list.append(np.array([0]))\n",
    "    peak_list.append(len(peaks))\n",
    "\n",
    "X_running = running_ecg\n",
    "y_running = peak_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X, y data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((X_walking, X_running, X_sitting))\n",
    "y = np.concatenate((y_walking, y_running, y_sitting))\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "# y = scaler.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "print(f\"\"\"X shape: {X.shape}\n",
    "y shape: {y.shape}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_idx = np.random.randint(0, len(X), 3)\n",
    "\n",
    "# Plotting the data\n",
    "fig, ax = plt.subplots(3, 1, figsize=(20, 10))\n",
    "\n",
    "for idx in range(3):\n",
    "    ax[idx].plot(X[plotting_idx[idx]])\n",
    "    ax[idx].set_title(f\"Peak count: {y[plotting_idx[idx]]}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('./data//exported/X.pkl', 'wb') as f:\n",
    "    pickle.dump(X, f)\n",
    "\n",
    "with gzip.open('./data//exported/y.pkl', 'wb') as f:\n",
    "    pickle.dump(y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('./data//exported/X.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "\n",
    "with gzip.open('./data//exported/y.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=datetime.datetime.now().microsecond)\n",
    "\n",
    "print(f\"\"\"X_train shape: {X_train.shape}\n",
    "y_train shape: {y_train.shape}\n",
    "X_test shape: {X_test.shape}\n",
    "y_test shape: {y_test.shape}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = np.array(X).astype(np.float32)\n",
    "        self.y = np.array(y).astype(np.float32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "train_dataset = ECGDataset(X_train, y_train)\n",
    "test_dataset = ECGDataset(X_test, y_test)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")\n",
    "EPOCH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=256):\n",
    "        super(PositionEmbedding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.position_embedding = nn.Embedding(max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        positions = torch.arange(0, x.size(1)).unsqueeze(0).repeat(x.size(0), 1).to(device)\n",
    "        return self.position_embedding(positions)\n",
    "    \n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, num_layers, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        return self.transformer_encoder(x, mask)\n",
    "    \n",
    "class InputConvolutions(nn.Module):\n",
    "    def __init__(self, d_model, kernel_size, stride, padding):\n",
    "        super(InputConvolutions, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        self.conv = nn.Conv1d(1, d_model, kernel_size, stride, padding)\n",
    "        self.conv1 = nn.Conv1d(d_model, d_model, 1, stride, padding)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(16384, d_model * 4)\n",
    "        self.linear3 = nn.Linear(d_model * 4, d_model)\n",
    "        self.linear4 = nn.Linear(d_model, 1)\n",
    "        self.linear5 = nn.Linear(1, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.linear3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.linear4(x)\n",
    "        x = torch.squeeze(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class RNN_Model(nn.Module):\n",
    "    def __init__(self, d_model, n_head):\n",
    "        super(RNN_Model, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.rnn = nn.GRU(d_model, d_model, n_head, batch_first=True)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.rnn(x)\n",
    "        return x\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, d_model: int, nhead: int, dim_feedforward: int, num_layers: int, dropout: float = 0.1):\n",
    "        super(Model, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.input_convolutions = InputConvolutions(d_model, 3, 1, 1)\n",
    "        self.position_embedding = PositionEmbedding(d_model)\n",
    "        self.transformer_encoder = TransformerEncoder(d_model, nhead, dim_feedforward, num_layers, dropout)\n",
    "        self.rnn_model = RNN_Model(d_model, nhead)\n",
    "        self.linear_model = RegressionModel(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_convolutions(x)\n",
    "        x = self.position_embedding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.rnn_model(x)\n",
    "        x = self.linear_model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    d_model=128,\n",
    "    nhead=8,\n",
    "    dim_feedforward=256,\n",
    "    num_layers=4,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "criterion = nn.SmoothL1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "train_loss, val_loss = [], []\n",
    "for epoch in range(EPOCH):\n",
    "    running_loss = 0.0\n",
    "    for i, data in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.reshape(-1, 1, 7500).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_loss.append(running_loss / len(train_dataloader))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(test_dataloader):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.unsqueeze(1).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        val_loss.append(running_loss / len(test_dataloader))\n",
    "    \n",
    "    model.train()\n",
    "    print(f\"Epoch: {epoch + 1} / {EPOCH} | Train loss: {train_loss[-1]:.5f} | Val loss: {val_loss[-1]:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_loss, label=\"Train loss\")\n",
    "plt.plot(val_loss, label=\"Val loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_dataloader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.unsqueeze(1).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        print(f\"Loss: {loss:.5f}\\t\\tPredicted: {outputs.item()}\\t\\tActual: {labels.item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
